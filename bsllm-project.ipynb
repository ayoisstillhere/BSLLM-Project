{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-22T10:59:10.483957Z",
     "iopub.status.busy": "2026-01-22T10:59:10.483725Z",
     "iopub.status.idle": "2026-01-22T10:59:16.487694Z",
     "shell.execute_reply": "2026-01-22T10:59:16.486776Z",
     "shell.execute_reply.started": "2026-01-22T10:59:10.483935Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q pandas torch transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T10:59:56.484705Z",
     "iopub.status.busy": "2026-01-22T10:59:56.483928Z",
     "iopub.status.idle": "2026-01-22T11:00:13.991928Z",
     "shell.execute_reply": "2026-01-22T11:00:13.991348Z",
     "shell.execute_reply.started": "2026-01-22T10:59:56.484671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T11:08:57.093443Z",
     "iopub.status.busy": "2026-01-22T11:08:57.092661Z",
     "iopub.status.idle": "2026-01-22T11:10:11.825765Z",
     "shell.execute_reply": "2026-01-22T11:10:11.824757Z",
     "shell.execute_reply.started": "2026-01-22T11:08:57.093399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: meta-llama/Meta-Llama-3-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0916bdb68cc24e24a817e0493e33d549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "login(token=\"MY_TOKEN\")\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T11:11:26.431362Z",
     "iopub.status.busy": "2026-01-22T11:11:26.430525Z",
     "iopub.status.idle": "2026-01-22T11:11:26.561962Z",
     "shell.execute_reply": "2026-01-22T11:11:26.561264Z",
     "shell.execute_reply.started": "2026-01-22T11:11:26.431329Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCQ Training Data Shape: (836, 7)\n",
      "MCQ Columns: ['MCQID', 'ID', 'country', 'prompt', 'choices', 'choice_countries', 'answer_idx']\n",
      "\n",
      "SAQ Training Data Shape: (1333, 6)\n",
      "SAQ Columns: ['ID', 'question', 'en_question', 'annotations', 'idks', 'country']\n",
      "\n",
      "MCQ Test Data Shape: (419, 5)\n",
      "SAQ Test Data Shape: (667, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_mcq = pd.read_csv(\"/kaggle/input/bsllm-project-data/train_dataset_mcq.csv\")\n",
    "train_saq = pd.read_csv(\"/kaggle/input/bsllm-project-data/train_dataset_saq.csv\")\n",
    "\n",
    "print(\"MCQ Training Data Shape:\", train_mcq.shape)\n",
    "print(\"MCQ Columns:\", train_mcq.columns.tolist())\n",
    "print(\"\\nSAQ Training Data Shape:\", train_saq.shape)\n",
    "print(\"SAQ Columns:\", train_saq.columns.tolist())\n",
    "\n",
    "# Load test data\n",
    "test_mcq = pd.read_csv(\"/kaggle/input/bsllm-project-data/test_dataset_mcq.csv\")\n",
    "test_saq = pd.read_csv(\"/kaggle/input/bsllm-project-data/test_dataset_saq.csv\")\n",
    "\n",
    "print(\"\\nMCQ Test Data Shape:\", test_mcq.shape)\n",
    "print(\"SAQ Test Data Shape:\", test_saq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T11:11:54.696538Z",
     "iopub.status.busy": "2026-01-22T11:11:54.695635Z",
     "iopub.status.idle": "2026-01-22T11:11:54.704597Z",
     "shell.execute_reply": "2026-01-22T11:11:54.703895Z",
     "shell.execute_reply.started": "2026-01-22T11:11:54.696505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_country_examples(df, target_country, n_examples=3):\n",
    "    \"\"\"Get examples from the same country for few-shot learning\"\"\"\n",
    "    country_data = df[df['country'] == target_country]\n",
    "    if len(country_data) < n_examples:\n",
    "        # If not enough examples from same country, get from any country\n",
    "        country_data = df\n",
    "    return country_data.sample(min(n_examples, len(country_data)))\n",
    "\n",
    "def build_few_shot_mcq_prompt(question_row, train_df, n_examples=3):\n",
    "    \"\"\"Build few-shot prompt for MCQ with examples\"\"\"\n",
    "    target_country = question_row['country']\n",
    "    examples = get_country_examples(train_df, target_country, n_examples)\n",
    "    \n",
    "    few_shot_text = \"\"\"You are an expert in global cultural knowledge with deep understanding of local customs and practices. Think step by step about the cultural context before answering.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add examples\n",
    "    for _, ex in examples.iterrows():\n",
    "        few_shot_text += f\"Question: {ex['prompt']}\\n\"\n",
    "        few_shot_text += f\"Correct Answer: {ex['answer_idx']}\\n\\n\"\n",
    "    \n",
    "    # Add the actual question\n",
    "    few_shot_text += f\"Now answer this question:\\n\\n{question_row['prompt']}\\n\"\n",
    "    few_shot_text += \"Think about the cultural context, then respond with ONLY the JSON format requested.\"\n",
    "    \n",
    "    return few_shot_text\n",
    "\n",
    "def build_few_shot_saq_prompt(question_row, train_df, n_examples=3):\n",
    "    \"\"\"Build few-shot prompt for SAQ with examples\"\"\"\n",
    "    target_country = question_row['country']\n",
    "    examples = get_country_examples(train_df, target_country, n_examples)\n",
    "    \n",
    "    few_shot_text = \"\"\"You are an expert in global cultural knowledge. Answer questions with specific, concise answers based on local cultural knowledge.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add examples with their top answers\n",
    "    for _, ex in examples.iterrows():\n",
    "        few_shot_text += f\"Question: {ex['en_question']}\\n\"\n",
    "        # Parse annotations to get the most common answer\n",
    "        try:\n",
    "            annotations = eval(ex['annotations'])\n",
    "            if annotations and len(annotations) > 0:\n",
    "                top_answer = annotations[0]['en_answers'][0]\n",
    "                few_shot_text += f\"Answer: {top_answer}\\n\\n\"\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Add the actual question\n",
    "    few_shot_text += f\"Now answer this question with a concise, specific answer:\\n\\n\"\n",
    "    few_shot_text += f\"Question: {question_row['en_question']}\\n\"\n",
    "    few_shot_text += \"Answer (be specific and concise):\"\n",
    "    \n",
    "    return few_shot_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T11:12:21.121362Z",
     "iopub.status.busy": "2026-01-22T11:12:21.120982Z",
     "iopub.status.idle": "2026-01-22T11:12:21.129700Z",
     "shell.execute_reply": "2026-01-22T11:12:21.129153Z",
     "shell.execute_reply.started": "2026-01-22T11:12:21.121335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def answer_mcq_single(prompt_text, temperature=0.7):\n",
    "    \"\"\"Generate a single MCQ answer\"\"\"\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return generated\n",
    "\n",
    "def extract_mcq_answer(generated_text):\n",
    "    \"\"\"Extract answer choice from generated text\"\"\"\n",
    "    # Try to extract JSON format first\n",
    "    json_match = re.search(r'\\{[^}]*\"answer_choice\"[^}]*:[^}]*\"([ABCD])\"[^}]*\\}', generated_text, re.IGNORECASE)\n",
    "    if json_match:\n",
    "        return json_match.group(1).upper()\n",
    "    \n",
    "    # Look for standalone letter\n",
    "    letter_match = re.search(r'\\b([ABCD])\\b', generated_text.upper())\n",
    "    if letter_match:\n",
    "        return letter_match.group(1)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def answer_mcq_with_consistency(question_row, train_df, n_samples=5):\n",
    "    \"\"\"Use self-consistency: generate multiple answers and take majority vote\"\"\"\n",
    "    prompt = build_few_shot_mcq_prompt(question_row, train_df, n_examples=2)\n",
    "    \n",
    "    answers = []\n",
    "    for i in range(n_samples):\n",
    "        generated = answer_mcq_single(prompt, temperature=0.7 if i > 0 else 0.1)\n",
    "        answer = extract_mcq_answer(generated)\n",
    "        if answer:\n",
    "            answers.append(answer)\n",
    "    \n",
    "    # If no valid answers, try one more time with different approach\n",
    "    if not answers:\n",
    "        # Direct approach without few-shot\n",
    "        direct_prompt = f\"\"\"{question_row['prompt']}\n",
    "\n",
    "Respond with ONLY the JSON format: {{\"answer_choice\":\"X\"}} where X is A, B, C, or D.\"\"\"\n",
    "        generated = answer_mcq_single(direct_prompt, temperature=0.1)\n",
    "        answer = extract_mcq_answer(generated)\n",
    "        if answer:\n",
    "            return answer\n",
    "        return 'A'  # Default fallback\n",
    "    \n",
    "    # Majority vote\n",
    "    answer_counts = Counter(answers)\n",
    "    return answer_counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-22T11:12:35.603560Z",
     "iopub.status.busy": "2026-01-22T11:12:35.603247Z",
     "iopub.status.idle": "2026-01-22T11:12:35.610338Z",
     "shell.execute_reply": "2026-01-22T11:12:35.609536Z",
     "shell.execute_reply.started": "2026-01-22T11:12:35.603533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def answer_saq_enhanced(question_row, train_df):\n",
    "    \"\"\"Generate answer for SAQ with few-shot prompting\"\"\"\n",
    "    prompt = build_few_shot_saq_prompt(question_row, train_df, n_examples=3)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=30,\n",
    "            do_sample=False,\n",
    "            temperature=0.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # Clean the answer\n",
    "    answer = generated.strip().split('\\n')[0].strip()\n",
    "    \n",
    "    # Remove common prefixes and artifacts\n",
    "    answer = re.sub(r'^(Answer:|A:|The answer is:?|Question:)\\s*', '', answer, flags=re.IGNORECASE)\n",
    "    answer = re.sub(r'^[\"\\']|[\"\\']$', '', answer)  # Remove quotes\n",
    "    answer = answer.strip()\n",
    "    \n",
    "    # Take only the first sentence/phrase if multiple exist\n",
    "    if '.' in answer:\n",
    "        answer = answer.split('.')[0]\n",
    "    if ',' in answer and len(answer) > 50:\n",
    "        answer = answer.split(',')[0]\n",
    "    \n",
    "    return answer.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Processing MCQ Test Data\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "mcq_predictions = []\n",
    "for idx, row in test_mcq.iterrows():\n",
    "    choice = answer_mcq_with_consistency(row, train_mcq, n_samples=5)\n",
    "    mcq_predictions.append(choice)\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(test_mcq)} MCQ questions\")\n",
    "\n",
    "# Create MCQ submission format\n",
    "test_mcq['choice'] = mcq_predictions\n",
    "mcq_submission = pd.get_dummies(test_mcq['choice'], dtype=bool)\n",
    "\n",
    "# Ensure all columns A, B, C, D exist\n",
    "for col in ['A', 'B', 'C', 'D']:\n",
    "    if col not in mcq_submission.columns:\n",
    "        mcq_submission[col] = False\n",
    "\n",
    "mcq_submission = pd.concat([test_mcq[['MCQID']], mcq_submission[['A', 'B', 'C', 'D']]], axis=1)\n",
    "mcq_submission.to_csv('mcq_prediction.tsv', sep='\\t', index=False)\n",
    "print(f\"\\nMCQ predictions saved! Total: {len(mcq_submission)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Processing SAQ Test Data\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "saq_predictions = []\n",
    "for idx, row in test_saq.iterrows():\n",
    "    answer = answer_saq_enhanced(row, train_saq)\n",
    "    saq_predictions.append(answer)\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(test_saq)} SAQ questions\")\n",
    "    \n",
    "    # Show some examples\n",
    "    if idx < 5:\n",
    "        print(f\"\\nQ: {row['en_question']}\")\n",
    "        print(f\"A: {answer}\")\n",
    "\n",
    "# Create SAQ submission format\n",
    "test_saq['answer'] = saq_predictions\n",
    "saq_submission = test_saq[['ID', 'answer']]\n",
    "saq_submission.to_csv('saq_prediction.tsv', sep='\\t', index=False)\n",
    "print(f\"\\nSAQ predictions saved! Total: {len(saq_submission)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Submission File\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with zipfile.ZipFile('submission.zip', 'w') as zipf:\n",
    "    zipf.write('saq_prediction.tsv')\n",
    "    zipf.write('mcq_prediction.tsv')\n",
    "\n",
    "print(\"✓ Submission file created: submission.zip\")\n",
    "print(\"\\nFiles included:\")\n",
    "print(\"  - mcq_prediction.tsv\")\n",
    "print(\"  - saq_prediction.tsv\")\n",
    "\n",
    "# Download files (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('submission.zip')\n",
    "    print(\"\\n✓ Submission file downloaded!\")\n",
    "except:\n",
    "    print(\"\\n✓ Submission file ready for download!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DONE! Your submission is ready.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
