{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-27T12:22:38.418304Z",
     "iopub.status.busy": "2026-01-27T12:22:38.417617Z",
     "iopub.status.idle": "2026-01-27T12:22:42.059229Z",
     "shell.execute_reply": "2026-01-27T12:22:42.058241Z",
     "shell.execute_reply.started": "2026-01-27T12:22:38.418259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q pandas torch transformers datasets peft accelerate bitsandbytes trl huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T12:22:44.746146Z",
     "iopub.status.busy": "2026-01-27T12:22:44.745821Z",
     "iopub.status.idle": "2026-01-27T12:22:55.890624Z",
     "shell.execute_reply": "2026-01-27T12:22:55.889787Z",
     "shell.execute_reply.started": "2026-01-27T12:22:44.746114Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-27 12:22:50.569714: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769516570.591543     140 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769516570.597959     140 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769516570.615497     140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769516570.615515     140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769516570.615517     140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769516570.615520     140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "import re\n",
    "import zipfile\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T12:23:01.409973Z",
     "iopub.status.busy": "2026-01-27T12:23:01.409670Z",
     "iopub.status.idle": "2026-01-27T12:23:01.593192Z",
     "shell.execute_reply": "2026-01-27T12:23:01.592452Z",
     "shell.execute_reply.started": "2026-01-27T12:23:01.409946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "login(token=\"MY_TOKEN\")\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "OUTPUT_DIR = \"./cultural-qa-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T12:23:03.650199Z",
     "iopub.status.busy": "2026-01-27T12:23:03.649379Z",
     "iopub.status.idle": "2026-01-27T12:23:03.688931Z",
     "shell.execute_reply": "2026-01-27T12:23:03.688040Z",
     "shell.execute_reply.started": "2026-01-27T12:23:03.650167Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Train MCQ: 836 samples\n",
      "Train SAQ: 1333 samples\n",
      "Test MCQ: 419 samples\n",
      "Test SAQ: 667 samples\n",
      "\n",
      "Country distribution in training data:\n",
      "MCQ: {'China': 219, 'US': 214, 'Iran': 206, 'UK': 197}\n",
      "SAQ: {'US': 340, 'CN': 339, 'GB': 331, 'IR': 323}\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading datasets...\")\n",
    "train_mcq = pd.read_csv(\"/kaggle/input/bsllm-project-data/train_dataset_mcq.csv\")\n",
    "train_saq = pd.read_csv(\"/kaggle/input/bsllm-project-data/train_dataset_saq.csv\")\n",
    "test_mcq = pd.read_csv(\"/kaggle/input/bsllm-project-data/test_dataset_mcq.csv\")\n",
    "test_saq = pd.read_csv(\"/kaggle/input/bsllm-project-data/test_dataset_saq.csv\")\n",
    "\n",
    "print(f\"Train MCQ: {len(train_mcq)} samples\")\n",
    "print(f\"Train SAQ: {len(train_saq)} samples\")\n",
    "print(f\"Test MCQ: {len(test_mcq)} samples\")\n",
    "print(f\"Test SAQ: {len(test_saq)} samples\")\n",
    "\n",
    "# Check country distribution\n",
    "print(\"\\nCountry distribution in training data:\")\n",
    "print(\"MCQ:\", train_mcq['country'].value_counts().to_dict())\n",
    "print(\"SAQ:\", train_saq['country'].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T12:23:05.794466Z",
     "iopub.status.busy": "2026-01-27T12:23:05.794093Z",
     "iopub.status.idle": "2026-01-27T12:23:06.045760Z",
     "shell.execute_reply": "2026-01-27T12:23:06.044989Z",
     "shell.execute_reply.started": "2026-01-27T12:23:05.794434Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing training examples...\n",
      "Total training examples: 3004\n",
      "  - MCQ: 836\n",
      "  - SAQ: 1304\n",
      "  - Oversampled China/Iran: 864\n"
     ]
    }
   ],
   "source": [
    "def create_mcq_training_example(row):\n",
    "    \"\"\"Convert MCQ row to instruction-following format\"\"\"\n",
    "    prompt = row['prompt']\n",
    "    answer_idx = row['answer_idx']\n",
    "    country = row['country']\n",
    "    \n",
    "    instruction = f\"\"\"You are an expert in {country} culture and customs. Answer the following multiple choice question accurately.\n",
    "\n",
    "{prompt}\"\"\"\n",
    "    \n",
    "    response = json.dumps({\"answer_choice\": answer_idx})\n",
    "    \n",
    "    # Llama-3 chat format\n",
    "    text = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a cultural expert specializing in global customs and practices.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{response}<|eot_id|>\"\"\"\n",
    "    \n",
    "    return {\"text\": text, \"country\": country}\n",
    "\n",
    "def create_saq_training_example(row):\n",
    "    \"\"\"Convert SAQ row to instruction-following format\"\"\"\n",
    "    question = row['en_question']\n",
    "    country = row['country']\n",
    "    \n",
    "    # Extract the top answer from annotations\n",
    "    try:\n",
    "        annotations = eval(row['annotations'])\n",
    "        if annotations and len(annotations) > 0:\n",
    "            # Get the most common answer\n",
    "            answer = annotations[0]['en_answers'][0].lower().strip()\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    instruction = f\"\"\"You are an expert in {country} culture and customs. Answer the following question with a concise, specific answer.\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "    \n",
    "    # Llama-3 chat format\n",
    "    text = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a cultural expert specializing in global customs and practices.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{instruction}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "{answer}<|eot_id|>\"\"\"\n",
    "    \n",
    "    return {\"text\": text, \"country\": country}\n",
    "\n",
    "# Create training examples\n",
    "print(\"\\nPreparing training examples...\")\n",
    "mcq_examples = [create_mcq_training_example(row) for _, row in train_mcq.iterrows()]\n",
    "saq_examples = [create_saq_training_example(row) for _, row in train_saq.iterrows()]\n",
    "saq_examples = [ex for ex in saq_examples if ex is not None]\n",
    "\n",
    "# Oversample China and Iran to balance the dataset\n",
    "china_iran_mcq = [ex for ex in mcq_examples if ex['country'] in ['China', 'IR']]\n",
    "china_iran_saq = [ex for ex in saq_examples if ex['country'] in ['CN', 'IR']]\n",
    "\n",
    "# Add oversampled examples (2x for underperforming countries)\n",
    "all_examples = mcq_examples + saq_examples + china_iran_mcq + china_iran_saq\n",
    "\n",
    "print(f\"Total training examples: {len(all_examples)}\")\n",
    "print(f\"  - MCQ: {len(mcq_examples)}\")\n",
    "print(f\"  - SAQ: {len(saq_examples)}\")\n",
    "print(f\"  - Oversampled China/Iran: {len(china_iran_mcq) + len(china_iran_saq)}\")\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame(all_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T12:23:09.689402Z",
     "iopub.status.busy": "2026-01-27T12:23:09.689043Z",
     "iopub.status.idle": "2026-01-27T12:23:13.069191Z",
     "shell.execute_reply": "2026-01-27T12:23:13.068195Z",
     "shell.execute_reply.started": "2026-01-27T12:23:09.689369Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.1)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (26.0rc2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T12:23:16.610089Z",
     "iopub.status.busy": "2026-01-27T12:23:16.609754Z",
     "iopub.status.idle": "2026-01-27T12:26:12.590635Z",
     "shell.execute_reply": "2026-01-27T12:26:12.589809Z",
     "shell.execute_reply.started": "2026-01-27T12:23:16.610057Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuring QLoRA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e15c5ada28a45f9844867017aa76391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552528116a1b425c94c9997111f820fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898eeca2a12f4ef39481e4256f71fc3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c820e243da24bda82170021ea0610c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02e46ecbfd245a6bcc3450b53388f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d811262083634773920adf365d181d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398ba39671fc4c1bbc3202b39c5571de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43613ebd77cc425698da27e04a77b528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d750e835c440e78e1355a2f7fb53e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f1dcde49de484496d969499097d00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6707c9bc06f4b1d879ccef74ff6ca48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Configure QLoRA (4-bit quantization + LoRA)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nConfiguring QLoRA...\")\n",
    "\n",
    "# 4-bit quantization config - NO bfloat16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Use float16 only\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,  # Force float16\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Training args - NO mixed precision\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  # Reduced batch size\n",
    "    gradient_accumulation_steps=8,  # Increased accumulation\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",  # Changed optimizer\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    # NO fp16 or bf16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T12:26:19.035667Z",
     "iopub.status.busy": "2026-01-27T12:26:19.034111Z",
     "iopub.status.idle": "2026-01-27T14:38:29.211221Z",
     "shell.execute_reply": "2026-01-27T14:38:29.210457Z",
     "shell.execute_reply.started": "2026-01-27T12:26:19.035628Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9921cd620c4f60a685a492bbb7e479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/3004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d547ee23fd4540b3092d49164e66c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/3004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c008054eb77d40a59bc34f1c193f9ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/3004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70bbd54e43b490faa8f80145617abe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/3004 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='564' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [564/564 2:11:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.973600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.170600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.700500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.456900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.436900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.388900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.330100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.292100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.289200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.258900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.243300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.225700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.196600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.190600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.179300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.177200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.162300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.123400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.125900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.131800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.121500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.128700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.129400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving fine-tuned model...\n",
      "âœ“ Model saved to ./cultural-qa-finetuned\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting fine-tuning...\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    args=training_args,\n",
    "    formatting_func=lambda x: x[\"text\"],\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "print(\"\\nSaving fine-tuned model...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"âœ“ Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T14:38:48.740457Z",
     "iopub.status.busy": "2026-01-27T14:38:48.740137Z",
     "iopub.status.idle": "2026-01-27T14:39:50.220468Z",
     "shell.execute_reply": "2026-01-27T14:39:50.219737Z",
     "shell.execute_reply.started": "2026-01-27T14:38:48.740430Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading fine-tuned model for inference...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988f89f48c974e86ac05b51dc009eae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Fine-tuned model loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading fine-tuned model for inference...\")\n",
    "\n",
    "# Reload base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load fine-tuned LoRA weights\n",
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "model.eval()\n",
    "\n",
    "print(\"âœ“ Fine-tuned model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T14:39:55.581598Z",
     "iopub.status.busy": "2026-01-27T14:39:55.580744Z",
     "iopub.status.idle": "2026-01-27T14:39:55.590709Z",
     "shell.execute_reply": "2026-01-27T14:39:55.589948Z",
     "shell.execute_reply.started": "2026-01-27T14:39:55.581561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_answer(prompt, max_new_tokens=150, temperature=0.1):\n",
    "    \"\"\"Generate answer using fine-tuned model\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return generated\n",
    "\n",
    "def answer_mcq_finetuned(row, n_samples=5):\n",
    "    \"\"\"Answer MCQ with self-consistency\"\"\"\n",
    "    country = row['country']\n",
    "    prompt_text = row['prompt']\n",
    "    \n",
    "    instruction = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a cultural expert specializing in global customs and practices.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "You are an expert in {country} culture and customs. Answer the following multiple choice question accurately.\n",
    "\n",
    "{prompt_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    answers = []\n",
    "    for i in range(n_samples):\n",
    "        temp = 0.7 if i > 0 else 0.1\n",
    "        generated = generate_answer(instruction, max_new_tokens=50, temperature=temp)\n",
    "        \n",
    "        # Extract answer\n",
    "        json_match = re.search(r'\\{[^}]*\"answer_choice\"[^}]*:[^}]*\"([ABCD])\"[^}]*\\}', generated, re.IGNORECASE)\n",
    "        if json_match:\n",
    "            answers.append(json_match.group(1).upper())\n",
    "        else:\n",
    "            letter_match = re.search(r'\\b([ABCD])\\b', generated.upper())\n",
    "            if letter_match:\n",
    "                answers.append(letter_match.group(1))\n",
    "    \n",
    "    if not answers:\n",
    "        return 'A'\n",
    "    \n",
    "    # Majority vote\n",
    "    answer_counts = Counter(answers)\n",
    "    return answer_counts.most_common(1)[0][0]\n",
    "\n",
    "def answer_saq_finetuned(row):\n",
    "    \"\"\"Answer SAQ question\"\"\"\n",
    "    country = row['country']\n",
    "    question = row['en_question']\n",
    "    \n",
    "    instruction = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a cultural expert specializing in global customs and practices.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "You are an expert in {country} culture and customs. Answer the following question with a concise, specific answer.\n",
    "\n",
    "Question: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    generated = generate_answer(instruction, max_new_tokens=30, temperature=0.1)\n",
    "    \n",
    "    # Clean answer\n",
    "    answer = generated.strip().split('\\n')[0].strip()\n",
    "    answer = re.sub(r'^(Answer:|A:|The answer is:?)\\s*', '', answer, flags=re.IGNORECASE)\n",
    "    answer = re.sub(r'^[\"\\']|[\"\\']$', '', answer)\n",
    "    \n",
    "    if '.' in answer:\n",
    "        answer = answer.split('.')[0]\n",
    "    \n",
    "    return answer.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T14:41:21.085940Z",
     "iopub.status.busy": "2026-01-27T14:41:21.085630Z",
     "iopub.status.idle": "2026-01-27T15:26:13.120441Z",
     "shell.execute_reply": "2026-01-27T15:26:13.119638Z",
     "shell.execute_reply.started": "2026-01-27T14:41:21.085914Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing MCQ Test Data with Fine-tuned Model\n",
      "================================================================================\n",
      "Processed 10/419 MCQ questions\n",
      "Processed 20/419 MCQ questions\n",
      "Processed 30/419 MCQ questions\n",
      "Processed 40/419 MCQ questions\n",
      "Processed 50/419 MCQ questions\n",
      "Processed 60/419 MCQ questions\n",
      "Processed 70/419 MCQ questions\n",
      "Processed 80/419 MCQ questions\n",
      "Processed 90/419 MCQ questions\n",
      "Processed 100/419 MCQ questions\n",
      "Processed 110/419 MCQ questions\n",
      "Processed 120/419 MCQ questions\n",
      "Processed 130/419 MCQ questions\n",
      "Processed 140/419 MCQ questions\n",
      "Processed 150/419 MCQ questions\n",
      "Processed 160/419 MCQ questions\n",
      "Processed 170/419 MCQ questions\n",
      "Processed 180/419 MCQ questions\n",
      "Processed 190/419 MCQ questions\n",
      "Processed 200/419 MCQ questions\n",
      "Processed 210/419 MCQ questions\n",
      "Processed 220/419 MCQ questions\n",
      "Processed 230/419 MCQ questions\n",
      "Processed 240/419 MCQ questions\n",
      "Processed 250/419 MCQ questions\n",
      "Processed 260/419 MCQ questions\n",
      "Processed 270/419 MCQ questions\n",
      "Processed 280/419 MCQ questions\n",
      "Processed 290/419 MCQ questions\n",
      "Processed 300/419 MCQ questions\n",
      "Processed 310/419 MCQ questions\n",
      "Processed 320/419 MCQ questions\n",
      "Processed 330/419 MCQ questions\n",
      "Processed 340/419 MCQ questions\n",
      "Processed 350/419 MCQ questions\n",
      "Processed 360/419 MCQ questions\n",
      "Processed 370/419 MCQ questions\n",
      "Processed 380/419 MCQ questions\n",
      "Processed 390/419 MCQ questions\n",
      "Processed 400/419 MCQ questions\n",
      "Processed 410/419 MCQ questions\n",
      "âœ“ MCQ predictions saved!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Processing MCQ Test Data with Fine-tuned Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "mcq_predictions = []\n",
    "for idx, row in test_mcq.iterrows():\n",
    "    choice = answer_mcq_finetuned(row, n_samples=5)\n",
    "    mcq_predictions.append(choice)\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(test_mcq)} MCQ questions\")\n",
    "\n",
    "# Create submission\n",
    "test_mcq['choice'] = mcq_predictions\n",
    "mcq_submission = pd.get_dummies(test_mcq['choice'], dtype=bool)\n",
    "\n",
    "for col in ['A', 'B', 'C', 'D']:\n",
    "    if col not in mcq_submission.columns:\n",
    "        mcq_submission[col] = False\n",
    "\n",
    "mcq_submission = pd.concat([test_mcq[['MCQID']], mcq_submission[['A', 'B', 'C', 'D']]], axis=1)\n",
    "mcq_submission.to_csv('mcq_prediction.tsv', sep='\\t', index=False)\n",
    "print(f\"âœ“ MCQ predictions saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T15:26:13.121917Z",
     "iopub.status.busy": "2026-01-27T15:26:13.121711Z",
     "iopub.status.idle": "2026-01-27T15:32:15.697690Z",
     "shell.execute_reply": "2026-01-27T15:32:15.697019Z",
     "shell.execute_reply.started": "2026-01-27T15:26:13.121896Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Processing SAQ Test Data with Fine-tuned Model\n",
      "================================================================================\n",
      "\n",
      "Q: What is the most popular children's animation that is commonly watched by kids in Iran?\n",
      "A: mia and dadi\n",
      "\n",
      "Q: What type of food from UK is typically served in restaurants overseas?\n",
      "A: fish & chips\n",
      "\n",
      "Q: What is the most popular food in China among young people?\n",
      "A: shaxian delicacies\n",
      "\n",
      "Q: What are the common activities that seniors usually do in parks in US?\n",
      "A: walk\n",
      "\n",
      "Q: Which major is considered most conducive to employment in China?\n",
      "A: computer science\n",
      "Processed 10/667 SAQ questions\n",
      "Processed 20/667 SAQ questions\n",
      "Processed 30/667 SAQ questions\n",
      "Processed 40/667 SAQ questions\n",
      "Processed 50/667 SAQ questions\n",
      "Processed 60/667 SAQ questions\n",
      "Processed 70/667 SAQ questions\n",
      "Processed 80/667 SAQ questions\n",
      "Processed 90/667 SAQ questions\n",
      "Processed 100/667 SAQ questions\n",
      "Processed 110/667 SAQ questions\n",
      "Processed 120/667 SAQ questions\n",
      "Processed 130/667 SAQ questions\n",
      "Processed 140/667 SAQ questions\n",
      "Processed 150/667 SAQ questions\n",
      "Processed 160/667 SAQ questions\n",
      "Processed 170/667 SAQ questions\n",
      "Processed 180/667 SAQ questions\n",
      "Processed 190/667 SAQ questions\n",
      "Processed 200/667 SAQ questions\n",
      "Processed 210/667 SAQ questions\n",
      "Processed 220/667 SAQ questions\n",
      "Processed 230/667 SAQ questions\n",
      "Processed 240/667 SAQ questions\n",
      "Processed 250/667 SAQ questions\n",
      "Processed 260/667 SAQ questions\n",
      "Processed 270/667 SAQ questions\n",
      "Processed 280/667 SAQ questions\n",
      "Processed 290/667 SAQ questions\n",
      "Processed 300/667 SAQ questions\n",
      "Processed 310/667 SAQ questions\n",
      "Processed 320/667 SAQ questions\n",
      "Processed 330/667 SAQ questions\n",
      "Processed 340/667 SAQ questions\n",
      "Processed 350/667 SAQ questions\n",
      "Processed 360/667 SAQ questions\n",
      "Processed 370/667 SAQ questions\n",
      "Processed 380/667 SAQ questions\n",
      "Processed 390/667 SAQ questions\n",
      "Processed 400/667 SAQ questions\n",
      "Processed 410/667 SAQ questions\n",
      "Processed 420/667 SAQ questions\n",
      "Processed 430/667 SAQ questions\n",
      "Processed 440/667 SAQ questions\n",
      "Processed 450/667 SAQ questions\n",
      "Processed 460/667 SAQ questions\n",
      "Processed 470/667 SAQ questions\n",
      "Processed 480/667 SAQ questions\n",
      "Processed 490/667 SAQ questions\n",
      "Processed 500/667 SAQ questions\n",
      "Processed 510/667 SAQ questions\n",
      "Processed 520/667 SAQ questions\n",
      "Processed 530/667 SAQ questions\n",
      "Processed 540/667 SAQ questions\n",
      "Processed 550/667 SAQ questions\n",
      "Processed 560/667 SAQ questions\n",
      "Processed 570/667 SAQ questions\n",
      "Processed 580/667 SAQ questions\n",
      "Processed 590/667 SAQ questions\n",
      "Processed 600/667 SAQ questions\n",
      "Processed 610/667 SAQ questions\n",
      "Processed 620/667 SAQ questions\n",
      "Processed 630/667 SAQ questions\n",
      "Processed 640/667 SAQ questions\n",
      "Processed 650/667 SAQ questions\n",
      "Processed 660/667 SAQ questions\n",
      "âœ“ SAQ predictions saved!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Processing SAQ Test Data with Fine-tuned Model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "saq_predictions = []\n",
    "for idx, row in test_saq.iterrows():\n",
    "    answer = answer_saq_finetuned(row)\n",
    "    saq_predictions.append(answer)\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(test_saq)} SAQ questions\")\n",
    "    \n",
    "    if idx < 5:\n",
    "        print(f\"\\nQ: {row['en_question']}\")\n",
    "        print(f\"A: {answer}\")\n",
    "\n",
    "# Create submission\n",
    "test_saq['answer'] = saq_predictions\n",
    "saq_submission = test_saq[['ID', 'answer']]\n",
    "saq_submission.to_csv('saq_prediction.tsv', sep='\\t', index=False)\n",
    "print(f\"âœ“ SAQ predictions saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-27T15:32:15.748008Z",
     "iopub.status.busy": "2026-01-27T15:32:15.747814Z",
     "iopub.status.idle": "2026-01-27T15:32:15.757255Z",
     "shell.execute_reply": "2026-01-27T15:32:15.756638Z",
     "shell.execute_reply.started": "2026-01-27T15:32:15.747989Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Creating Submission File\n",
      "================================================================================\n",
      "âœ“ Submission file created: submission_finetuned.zip\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_f183675b-0306-41d3-8a96-cf0d09d69110\", \"submission_finetuned.zip\", 27343)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Submission downloaded!\n",
      "\n",
      "================================================================================\n",
      "FINE-TUNING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Expected improvements:\n",
      "  MCQ: 0.71 â†’ 0.78-0.82\n",
      "  SAQ: 0.58 â†’ 0.68-0.73\n",
      "  China/Iran performance should improve significantly\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Submission File\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with zipfile.ZipFile('submission_finetuned.zip', 'w') as zipf:\n",
    "    zipf.write('saq_prediction.tsv')\n",
    "    zipf.write('mcq_prediction.tsv')\n",
    "\n",
    "print(\"âœ“ Submission file created: submission_finetuned.zip\")\n",
    "\n",
    "# Download files\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('submission_finetuned.zip')\n",
    "    print(\"\\nâœ“ Submission downloaded!\")\n",
    "except:\n",
    "    print(\"\\nâœ“ Submission ready for download!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINE-TUNING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nExpected improvements:\")\n",
    "print(\"  MCQ: 0.71 â†’ 0.78-0.82\")\n",
    "print(\"  SAQ: 0.58 â†’ 0.68-0.73\")\n",
    "print(\"  China/Iran performance should improve significantly\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9313853,
     "sourceId": 14580545,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
